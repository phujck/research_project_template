---
citekey: "Kim2024"
title: "Long-Tailed Recognition on Binary Networks by Calibrating A Pre-trained Model"
authors: ['Jihun Kim', 'Dahyun Kim', 'Hyungrok Jung', 'Taeil Oh', 'Jonghyun Choi']
year: 2024
venue: "arXiv"
status: "Inbox"
tags: []
---

# Abstract
Deploying deep models in real-world scenarios entails a number of challenges, including computational efficiency and real-world (e.g., long-tailed) data distributions. We address the combined challenge of learning long-tailed distributions using highly resource-efficient binary neural networks as backbones. Specifically, we propose a calibrate-and-distill framework that uses off-the-shelf pretrained full-precision models trained on balanced datasets to use as teachers for distillation when learning binary networks on long-tailed datasets. To better generalize to various datasets, we further propose a novel adversarial balancing among the terms in the objective function and an efficient multiresolution learning scheme. We conducted the largest empirical study in the literature using 15 datasets, including newly derived long-tailed datasets from existing balanced datasets, and show that our proposed method outperforms prior art by large margins (>14.33% on average).

# key Findings
(To be filled)

# Methodology
(To be filled)
